"""
ì›¹ ì½˜í…ì¸  ì¶”ì¶œ ëª¨ë“ˆ
Trafilaturaë¥¼ ì‚¬ìš©í•˜ì—¬ URLì—ì„œ ë³´ëˆ”ã„´ê³¼ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
"""

from trafilatura import fetch_url, extract, extract_metadata
from urllib.parse import urlparse
from utils.logging import logger
from typing import Optional, Dict, Any
import requests

def extract_content(url: str) -> Optional[Dict[str, Any]]:
    """
    URLì—ì„œ ì½˜í…ì¸ ì™€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

    Args:
        url: ì¶”ì¶œí•  ì›¹í˜ì´ì§€ URL

    Return:
        Dict with:
        - url : URL
        - domain: ë„ë©”ì¸
        - title : ì œëª©
        - author : ì €ì
        - date : ë°œí–‰ì¼
        - content: ë³¸ë¬¸
        
        ì‹¤íŒ¨ ì‹œ None
    """
    # logger.info(f"ì½˜í…ì¸  ì¶”ì¶œ ì‹œì‘: {url}")

    try:
        # ê¸°ë³¸ ì¶”ì¶œ
        # Cloudflare ë³´í˜¸ë¥¼ trafilaturaìœ¼ë¡œ í•´ê²°í•˜ê¸° ì–´ë ¤ì›Œì„œ requests ë³‘ìš©
        # html = fetch_url(url)
        headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120.0.0.0 Safari/537.36"
            )
        }
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            return None

        html = response.text

        # ë³¸ë¬¸ ì¶”ì¶œ
        content = extract(
            html,
            include_comments=False,   # ëŒ“ê¸€ ì œì™¸
            include_tables=True,      # í‘œ í¬í•¨
            no_fallback=False         # í’€ë°± í—ˆìš© (ì¶”ì¶œ ì •í™•ë„ í–¥ìƒ)
        )

        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata = extract_metadata(html)

        # ë„ë©”ì¸ ì¶”ì¶œ
        domain = urlparse(url).netloc

        # ê²°ê³¼ êµ¬ì„±
        result = {
            "url": url,
            "domain": domain,
            "title": metadata.title if metadata else None,
            "author": metadata.author if metadata else None,
            "date": metadata.date if metadata else None,
            "content": content,
        }

        logger.info(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {result['title']}")
        logger.debug(f"   ë„ë©”ì¸: {domain}")

        return result
    
    except Exception as e:
        logger.error(f"âŒ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        logger.exception(e)
        return None
    

def detect_source_type(url: str) -> str:
    """
    URL ê¸°ë°˜ìœ¼ë¡œ ì½˜í…ì¸  ì†ŒìŠ¤ ìœ í˜•(blog, docs, news, forum, youtube, article ë“±)ì„ ê°ì§€í•©ë‹ˆë‹¤.
    ë‹¨ìˆœ íŒ¨í„´ ë§¤ì¹­ + ë„ë©”ì¸ ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¥˜.
    
    Args:
        url: ì›¹í˜ì´ì§€ URL

    Returns:
        source_type: 'youtube', 'blog', 'docs', 'article', 'unknown'
    """
    url_lower = url.lower()
    domain = urlparse(url).netloc.lower()

    # --- YouTube / ì˜ìƒ ---
    VIDEO_DOMAINS = [
        "youtube.com", "youtu.be", "vimeo.com", "navertv.", "dailymotion.com",
        "twitch.tv", "kakao.tv", "afreecatv.com", "ted.com/talks",
        "bilibili.com", "inflearn.com", "fastcampus.co"
    ]
    if any(d in domain for d in VIDEO_DOMAINS) or "/video/" in url_lower:
        return "video"

    # --- ë¸”ë¡œê·¸ ---
    BLOG_DOMAINS = [
        "tistory.com", "velog.io", "medium.com", "brunch.co.kr", "naver.com",
        "notion.site", "substack.com", "hashnode.dev", "ghost.io",
        "wordpress.com", "blogspot.com", "dev.to", "teletype.in",
        "post.naver.com", "mirror.xyz"
    ]
    if any(d in domain for d in BLOG_DOMAINS) or "/blog/" in url_lower:
        return "blog"

    # --- ë¬¸ì„œ / ê¸°ìˆ  ë¬¸ì„œ ---
    DOC_DOMAINS = [
        "readthedocs.io", "github.io", "docsify", "developer.", "api.",
        "python.langchain.com", "docs.", "devdocs.io", "notion.com",
        "learn.microsoft.com", "developer.mozilla.org", "pkg.go.dev",
        "pytorch.org", "tensorflow.org", "react.dev", "vuejs.org"
    ]
    if any(d in domain for d in DOC_DOMAINS) or "/docs/" in url_lower or "/guide/" in url_lower:
        return "docs"

    # --- ë‰´ìŠ¤ / ë¯¸ë””ì–´ ---
    NEWS_DOMAINS = [
        "news.naver.com", "bbc.com", "nytimes.com", "cnn.com",
        "reuters.com", "bloomberg.com", "theguardian.com",
        "ytn.co.kr", "mbc.co.kr", "sbs.co.kr", "kbs.co.kr",
        "hani.co.kr", "chosun.com", "joongang.co.kr", "donga.com"
    ]
    if any(d in domain for d in NEWS_DOMAINS) or "/news/" in url_lower:
        return "news"

    # --- ì»¤ë®¤ë‹ˆí‹° / Q&A ---
    FORUM_DOMAINS = [
        "reddit.com", "stackoverflow.com", "okky.kr", "ruliweb.com",
        "clien.net", "slack.com", "discord.com", "github.com/issues",
        "medium.com/@", "cafe.naver.com"
    ]
    if any(d in domain for d in FORUM_DOMAINS) or "/questions/" in url_lower:
        return "forum"

    # --- ê¸°ë³¸ê°’: ì¼ë°˜ ê¸°ì‚¬ ë˜ëŠ” ê¸°íƒ€ ì½˜í…ì¸  ---
    return "article"

# í…ŒìŠ¤íŠ¸ìš©
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ URL
    test_urls = [
        "https://trafilatura.readthedocs.io/en/latest/downloads.html"
    ]
    
    print("ğŸ§ª Extractor í…ŒìŠ¤íŠ¸\n")
    print("=" * 60)
    
    for url in test_urls:
        print(f"\nğŸ“„ í…ŒìŠ¤íŠ¸: {url}")
        print("-" * 60)
        
        # ì†ŒìŠ¤ íƒ€ì…
        source_type = detect_source_type(url)
        print(f"ğŸ“ ì†ŒìŠ¤ íƒ€ì…: {source_type}")
        
        # ì½˜í…ì¸  ì¶”ì¶œ
        result = extract_content(url)
        
        if result:
            print(f"âœ… ì¶”ì¶œ ì„±ê³µ!")
            print(f"   ì œëª©: {result['title']}")
            print(f"   ë„ë©”ì¸: {result['domain']}")
            print(f"   ì†ŒìŠ¤ ìœ í˜•: {source_type}")
            print(f"   ì €ì: {result['author']}")
            print(f"   ë‚ ì§œ: {result['date']}")
            print(f"   ë³¸ë¬¸ ê¸¸ì´: {len(result['content'])} chars")
            print(f"\n   ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°:")
            print(f"   {result['content'][:150]}...")
        else:
            print(f"âŒ ì¶”ì¶œ ì‹¤íŒ¨!")
        
        print()
    
    print("=" * 60)
    print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")